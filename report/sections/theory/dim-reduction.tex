\section{Dimensionality Reduction} \label{sect:theory:dim-reduction}
% What is dimensionality?
% What is the curse of dimensionality?
In machine learning and statistics, it is common to refer to the number of features that make up an observation as its dimensionality. As the number of features that describe such observation increases, it is likely that one will encounter the so called ``curse of dimensionality''. The curse of dimensionality is the manifestation of all phenomena that occurs when dealing with high--dimensional data, and that have most often unfortunate consequences on the behavior and performances of learning algorithms \cite{article:curse-of-dim}. \newline

% What is dimensionality reduction?
Dimensionality reduction refers to the process of reducing the number of features that describe an observation. Dimensionality reduction can be performed by either using feature selection (selecting a subset of the original features) or feature extraction (deriving new features from the original features). Dimensionality reduction can help avoid the curse of dimensionality, eliminate unsuitable features, reduce noise, and reduce the amount of time and memory required by machine learning or statistical algorithms to execute. \newline

% How does this implementation of the ML-Blink algorithm do dimensionality reduction?
The implementation of the \mlblink algorithm in this report uses a linear inner product to project a pair of items $(\vect{x}_i, \vect{y}_j)$ to a lower dimension. The choice to use an inner product as a dimensionality reduction technique was made due to its simplicity, performance, and the results it was able to achieve. \newline

To better illustrate this method, consider a vector $\vect{x}_i$ where $n_x=3$ and a matrix $\vect{P}$ of size $3 \times 9$ as in equation \ref{eq:proj}.

\begin{equation} \label{eq:proj}
    \begin{split}
        \vect{p} 
        &= 
            \vect{x}_{i} \cdot \vect{P} \\
        &=
            \vect{x}_{i} \cdot
            \begin{bmatrix}
                1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
                0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
                0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
            \end{bmatrix} \\  
        &=
            \begin{bmatrix}
                x_{i,1} + x_{i,2} + x_{i,3} & x_{i,4} + x_{i,5} + x_{i,6} & x_{i,7} + x_{i,8} + x_{i,9}
            \end{bmatrix} \\
    \end{split}
\end{equation}

As shown in equation \ref{eq:proj}, the resulting vector $\vect{p}$ has only 3 dimensions. Each of these dimensions was created by adding a vector component and the next two consecutive components next to it until all elements in the initial vector $\vect{x}_i$ were processed. The linear inner product dimensionality reduction technique is essentially a form of feature extraction, as new features of the vector $\vect{x}_i$ were derived from its original components.